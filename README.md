# DailyComputerScienceChallenge

## 25. 11. 12(수)
### 오늘의 질문: JWT(JSON Web Token)란 무엇이고, 왜 사용하나요? 


## 25. 11. 13(목)
### 오늘의 질문: 캐시(Cache)란 무엇이고, 왜 사용하나요?

**캐시란? 자주 조회되는 데이터를 임시로 저장해두는 공간**

원본은 DB 또는 디스크 와 같은 저장공간에 있지만, 읽기를 빠르게 하기 위해 메모리(RAM, 메모리, CPU 캐시 등)에 미리 저장해두는 방식

### 왜 캐시를 사용할까?

DB 접근 방식보다 압도적으로 빠르기 때문

DB -> 네트워크 + 디스크 접근 → ms 단위

캐시 -> 메모리 접근 = µs 단위 (마이크로초)

### 추가질문 1 : 캐시가 빠른 원리는?
**1. 메모리는 디스크보다 구조적으로 빠름**
* RAM 전기 신호로 즉시 접근가능
* DB는 디스크 I/O + 인덱스 탐색 + 네트워크 왕복이 필요함 상대적으로 느림

**2. 네트워크 이동이 필요없음**
* Redis와 같은 캐시는 "인메모리 저장소"이기에 요청, 응답이 간단함
**  3. 자주 쓰는 데이터를 미리 가져다 두기 때문에**
* DB에 매번 요청할 필요가 없어서 지연이 줆어듬

### 추가질문 2 : 언제 캐시를 사용하면 좋을까?

**1. 조회가 매우 많지만, 값이 자주 안 바뀌는 데이터**

     ex) 인기 게시글 목록, 상품 정보, 설정값, 공지사항 등
         -> 읽기는 많고 쓰기는 적은 경우

**2. DB 부하가 높을 때**

     트래픽이 몰리면 DB 병목 현상 발생 할 수 있음
     But 캐시가 있으면 대부분의 요청이 DB로 가지 않고 캐시에서 처리됨

**3. 외부 API 호출이 느릴 때**

    ex) 환율, 날씨 등
         -> 매번 API를 호출시 느림 한번 가져온 값을 캐싱하여 효율화

**4. 세션 저장소**

    로그인 세션을 Redis에 저장 -> 빠르고 분산 서버 간 공유 가능

### 캐시의 예시
1. Redis (가장 유명한 인메모리 캐시)
2. Memcached
3. CDN 캐시 (CloudFront, Akamai 등)
4. CPU L1/L2 Cache

### 캐시의 흐름
```
[사용자 요청]
      ↓
  캐시 확인 (Redis)
      ↓
  ✓ 있으면?  → 바로 응답 (super fast)
  ✗ 없으면?  → DB 조회 → 캐시에 저장 → 응답
```
### 요약
  캐시는 "자주 쓰는 데이터를 더 빠른 메모리에 저장해, 시스템 속도와 안정성을 극대화하는 기술"


## 25. 11. 14(금)
### 오늘의 질문: 로드 밸런싱(Load Balancing)이란 무엇이고, 왜 필요한가요?

**로드 밸런싱(Load Balancing)이란? 여러 대의 서버가 있을 때, 들어오는 요청을 적절히 분배해주는 시스템 또는 기술**

### 왜 로드 밸런싱이 필요한가?
**1. 트래픽 폭주(서버 과부화 방지)**
* 사용자가 몰리면 요청을 분산하여 어느 한 서버가 터지지 않게 방지함

**2. 고가용성(High Availability) 보장**
* 서버 하나가 다운되어도 나머지 서버로 자동 분산 -> 서비스는 계속 정상 작동

**3. 확장성(Scalability) 확보**
* 서버를 쉽게 확장 가능함 (2대 -> 4대 -> 10대)
* 로드 밸런서가 있으면 확장/축소에 용이함

**4. 빠른 응답**
* 가장 가까운 서버, 가정 적게 부하가 걸린 서버로 보내서 성능을 최적화 할 수 있음

### 로드 밸런서의 핵심 역할
**1. 트래픽 분산 (가장 핵심적인 역할)**
* 요청을 어떤 서버로 보낼지 결정

**2. 헬스 체크(Health Check)**
* 서버가 살아있는지 정기적으로 확인
* 죽은 서버는 트래픽 분배 대상에서 제외함

**3. 장애 자동 대응(Failover)**
* 서버가 하나 다운시 자동으로 다른 서버로 우회

**4. SSL 종료(SSL Offloading)**
* HTTPS 인증서 처리 부담을 LB가 해줌
* 실제 서버는 HTTP로 빠르게 처리

**5. 세션 스티키(Session Stickiness)**
* 특정 사용자를 계속 같은 서버로 보내는 기능
* 로그인 세션 있는 서비스에서 사용됨


### 추가질문 1 : 트래픽을 어떻게 나눌까?(분산 방식)
**1. 라운드 로빈(Round Robin)**
* 순차적으로 1번 -> 2번 -> 3번 -> 1번 ....
* 가장 심플하고 많이 사용됨

**2. 가중 라운드 로빈(Weighted Round Robin)**
* 서버 스펙이 다르면 더 성능이 좋은 서버에 비중을 높여서 분배
ex) Sever A(8코어), Sever(4코어) -> A에 2배 트래픽

**3) 최소 연결 방식(Least Connections)**
* 현재 **연결(요청)** 이 가장 적은 서버에게 보냄
* 실시간 트래픽 균형에 강함

**4. IP 해시 방식(IP Hash)**
* 사용자의 IP에 기반해 항상 같은 서버로 보냄
* 로그인 세션 유지에 좋음

### 추가질문 2 : 사용자가 많이 몰릴 경우 어떻게 해야 하나요?
1. 서버 여러 대를 준비

EC2, 컨테이너, 온프레미스 서버 등

2. 로드 밸런서를 앞단에 둔다

AWS ALB / NLB, Nginx, HAProxy 등이 대표적

 3. 오토스케일링 연계

트래픽이 증가하면 자동으로 서버 추가

감소하면 자동 감축 → 비용 최적화

AWS에서의 구조
```
사용자 → ALB → EC2(or EKS PODs) 여러 대
              ↑
        Auto Scaling Group
```
### 실제 서비스에서 동작 방식
1. 사용자가 웹사이트 접속

2. ALB가 헬스 체크하며 정상 서버 찾기

3. 분산 전략에 따라 서버로 요청 전달

4. 서버가 응답

5. ALB가 사용자에게 응답 반환

### 요약
**로드 밸런싱은 트래픽을 여러 서버에 분산해 서버 과부하를 방지하고, 서비스 속도·안정성을 높이는 핵심 기술**


## 25. 11. 17(월)
### 오늘의 질문: CORS(Cross-Origin Resource Sharing)란 무엇이고, 왜 발생하나요?



## 25. 11. 18(화)
### 오늘의 질문: 메시지 큐(Message Queue)란 무엇이고, 언제 사용하나요?


## 25. 11. 19(수)
### 오늘의 질문: CI/CD란 무엇이고, 왜 필요한가요?

### CI/CD란?
**CI(Continuous Integration, 지속적 통합)**

개발자들이 작성한 코드를 자동으로 빌드하고 테스트해서, "잘 합쳐졌는지 문제가 없는지"를 체크하는 과정

**CD(Continuous Delivery / Continuous Deployment, 지속적 배포)**

검증된 코드를 자동으로 배포하거나 배포 직전 상태까지 준비하는 과정

* CI = 통합  + 테스트 자동화
* CD = 배포 자동화

### CI/CD는 왜 필요한가?

이유는 하나 : 사람이 하면 실수한다

### 그래서 장점은? 

1. 배포 속도가 빨라진다

* 매번 수동으로 빌드/테스트/배포할 필요 없음
* 코드 푸시 → 몇 분 안에 자동 빌드 + 테스트 + 배포

2. 품질이 좋아진다

* 자동 테스트로 버그를 일찍 발견
* 잘못된 코드가 운영에 올라갈 확률이 줄어듦

3. 협업이 쉬워진다

* 여러 사람이 동시에 개발해도 충돌 위험이 줄어듦
* PR마다 자동 테스트 → 코드 품질 일정하게 유지

4. 배포가 안정적이고 재현 가능

* "어제 배포는 됐는데 오늘은 왜 안 되지?" 같은 문제 사라짐
* 파이프라인을 통해 항상 같은 방식으로 빌드/배포됨

5. 장애 대응 속도가 빨라진다

* 핫픽스(push) → 자동 빌드 → 자동 배포
* 몇 분 안에 수정사항 반영 가능

### CI/CD의 흐름
```
1. (CI) GitHub Actions가 실행  
2. 코드 빌드  
3. 테스트 실행  
4. Docker 이미지 빌드  
5. 이미지 ECR로 푸시  
6. (CD) ArgoCD가 변경 감지  
7. Kubernetes(EKS)에 자동 배포
```

### CI/CD에서 사용하는 대표적인 도구
✔ CI 도구

* GitHub Actions (가장 많이 씀)
* Jenkins
* GitLab CI
* CircleCI

✔ CD 도구

* ArgoCD (GitOps 방식)
* GitHub Actions + CodeDeploy
* Spinnaker

### 요약

CI/CD는 개발 속도와 안정성을 극대화하는 “개발·운영 자동화 파이프라인”이다.


## 25. 11. 20(목)
### 오늘의 질문: 비지 컬렉션(Garbage Collection)이란 무엇이고, 어떻게 동작하나요?

### 가비지 컬렉션(GC)이란?
**프로그램이 사용이 끝난 객체를 자동으로 찾아서 메모리를 헤제해주는 기능**
Java, Python, Go 같은 언어는 GC가 자동으로 관리하는 “Managed Language”

C, C++ 같은 언어는 개발자가 직접 메모리를 관리하는 “Unmanaged Language”

### GC는 어떻게 동작할까?
**핵심 개념: Reachability(도달 가능성)**

GC는 현재 프로그램에서 참조할 수 있는 객체인지를 기준으로 판단

* 도달 가능한 객체 → 사용 중 → 삭제 X
* 도달 불가능한 객체 → 더 이상 필요 없는 객체 → 삭제 대상 O

즉, 어떤 변수나 실행 스택에서 **참조가 끊긴 객체**는 자동으로 정리됨.

### 요약
**GC는 결국 “자동 메모리 최적화 시스템”**

GC = 계속 쓰이는 메모리는 유지하고,
더 이상 쓰지 않는 메모리는 자동으로 청소해서 프로그램의 안정성을 지켜주는 조력자

## 25. 11. 21(금)
### 오늘의 질문: API Gateway란 무엇이고, 왜 사용하나요?

### 1. API Gateway란?
여러 개의 백엔드 서비스(MSA)를 하나의 통합된 API 엔드포인트로 묶어, 클라이언트가 단일 창구로 요청을 보내도록 하는 시스템

즉, 클라이언트는

User 서비스, Order 서비스, Payment 서비스, Product 서비스
이렇게 나눠져 있는지 알 필요가 없음

/api/... 하나로 요청하면 API Gateway가 알아서 적절한 백엔드로 라우팅 진행

### 2. 왜 API Gateway가 필요한가
MSA 환경에서는 API Gateway 없이는 문제가 매우 많이 발생하기 때문

API Gateway가 없으면

문제 ① 클라이언트가 모든 서비스를 직접 호출해야 함

예:
프론트엔드가 User 서비스, Post 서비스, Auth 서비스… 등
10개, 20개의 엔드포인트를 알아야 함.

→ 유지보수 지옥 

→ 모바일에서는 특히 네트워크 연결 과부하

문제 ② 각 서비스마다 인증/인가를 직접 구현해야 함

→ 로직 중복

→ 보안정책 통일 불가능

→ 버그 발생률 ↑

문제 ③ 서비스 URL 변경 시 클라이언트도 수정 필요

운영 중 서비스가 옮겨지면 → 앱/웹 클라이언트 모두 업데이트해야 함

문제 ④ 모니터링/로깅/트래픽 제어가 서비스별로 분산

→ 장애 원인을 찾기 어려움

→ 일관된 정책 적용 불가능

### 3. API Gateway가 제공하는 기능
✔ 1) 라우팅(Routing)

클라이언트 요청 → 해당 백엔드 서비스로 전달

예:

/users/** → User Service

/orders/** → Order Service

✔ 2) 인증·인가 처리(Authentication & Authorization)

JWT 검증, OAuth2, API Key 처리 등

→ 서비스 내부에서는 인증 부담 사라짐

✔ 3) Rate Limiting (요청 제한)

봇 공격, 트래픽 폭주 방지

예: IP당 1초에 10개의 요청만 허용

✔ 4) 로깅 & 모니터링

스펙 분산 없이 한 곳에서 관제

CloudWatch, ELK, Prometheus 등과 연계

✔ 5) 캐싱

자주 쓰는 응답을 Gateway에서 캐싱 → 백엔드 부하 대폭 감소

✔ 6) 트래픽 관리(로드 밸런싱)

Gateway → 여러 백엔드 서버로 분산

### 4. API Gateway를 사용했을 때 통신 흐름
```
[Client] → [API Gateway] → [Auth Service]
                         → [User Service]
                         → [Order Service]
                         → [Product Service]
```


클라이언트는 단 하나의 URL만 알면 됨.

### 5. API Gateway의 대표적인 종류

**AWS API Gateway**

Serverless API 관리에 최적

Lambda, DynamoDB와 자주 함께 사용

**Kong**

오픈소스 기반 API Gateway 1위

**NGINX Gateway**

고성능 트래픽 처리에 강함

**Spring Cloud Gateway**

Spring 기반 MSA에서 사용하기 매우 편리

**Istio의 Ingress Gateway**

서비스 메시(Service Mesh)의 Gateway 역할

### 6. 언제 API Gateway가 especially 중요할까?

MSA 구조일 때

모바일/웹 클라이언트가 여러 서비스에 접근할 때

인증/보안 정책을 중앙화해야 할 때

API 버저닝이 필요한 경우

모니터링을 통합하고 싶을 때

즉, **서비스 규모가 커지면 API Gateway는 거의 필수요소**

### 한 문장 요약

API Gateway는 모든 백엔드 서비스를 하나의 관문으로 묶어,
인증·라우팅·보안·로깅·트래픽 제어를 중앙화하는 필수 인프라 요소이다.

## 25. 11. 21(금)
### 오늘의 질문: 페이징(Paging)과 세그멘테이션(Segmentation)의 차이점은 무엇인가요?

### 1. 페이징(Paging)이란?

메모리를 동일한 크기의 “페이지(Page)” 단위로 나누어 관리하는 방식

논리 주소 공간(Logical Address Space)은 여러 개의 페이지(Page)로 구성

물리 메모리(Physical Memory)는 같은 크기의 프레임(Frame)으로 구성

운영체제는 “어떤 페이지가 물리 메모리의 어떤 프레임에 있는지”만 매핑하면 됨

**핵심 아이디어**

고정 크기(Fixed-size) 블록 단위로 나누면 관리가 쉽고, 메모리 단편화 문제를 거의 해결한다.

✔ 장점

외부 단편화 없음(모두 동일 크기라 빈 공간이 잘 생기지 않음)

물리 메모리 비연속 배치 가능 → 메모리 효율 ↑

페이지 교체 알고리즘(LRU 등)과 결합해 가상메모리 구현 가능

❌ 단점

내부 단편화 발생 가능
(마지막 페이지가 다 차지 않으면 남는 공간이 낭비됨)

페이지 테이블이 커짐 → 메모리 사용량 증가

캐시 미스 + 페이지 테이블 접근 → 주소 변환 오버헤드(TLB 필요)

### 2. 세그멘테이션(Segmentation)이란?

프로그램을 의미 있는 논리적 단위(세그먼트 단위)로 나누는 방식

예:

코드 세그먼트

데이터 세그먼트

스택 세그먼트

힙 세그먼트

전역 변수 영역 …

각 세그먼트는 크기가 모두 다름(Variable-size)
그리고 의미 단위로 나뉘기 때문에 개발/관리 측면에서 자연스러움

✔ 장점

의미 단위로 나뉘어 관리 → 보호와 공유가 쉬움

예: 코드 영역은 read-only로, 데이터 영역은 read/write 가능

모듈 단위 메모리 할당에 유리

실제 프로그램 구조와 잘 맞음

❌ 단점

외부 단편화 발생
(크기가 서로 달라 빈 공간 생김)

세그먼트가 커지면 연속적인 큰 공간이 필요

메모리 배치/재배치 비용 ↑

### 3. 페이징 vs 세그멘테이션 (차이 한눈에 비교)
| 항목     | 페이징 (Paging)         | 세그멘테이션 (Segmentation) |
| ------ | -------------------- | --------------------- |
| 단위 크기  | 고정 크기(Page)          | 가변 크기(Segment)        |
| 나누는 기준 | **물리적인 크기 기준**       | **논리적 의미 기준**         |
| 단편화 문제 | 내부 단편화 가능, 외부 단편화 없음 | 내부 단편화 없음, 외부 단편화 발생  |
| 보호/공유  | 페이지 단위로 가능하지만 관리 복잡  | 코드/데이터 단위로 자연스럽게 가능   |
| 메모리 배치 | 물리 메모리 비연속적 가능       | 연속 공간이 필요(기본 세그멘트 방식) |
| 구현 복잡도 | 낮음                   | 높음                    |

###  4. 운영체제는 실제로 어떤 방식으로 사용할까?

현대 OS는 페이징 + 세그멘테이션 혼합 또는 페이징 기반 가상 메모리를 사용해요.

📍 예: x86 아키텍처

세그멘테이션으로 논리 공간을 나눔

페이징으로 물리 메모리 매핑
→ 세그멘테이션은 거의 보호용으로만 사용, 실제 메모리 관리는 페이징 중심

📍 예: Linux

세그멘테이션 기능 거의 사용 X

완전한 페이징 기반 가상 메모리

페이지 단위 swap, demand paging, huge page 지원

###  5. 메모리 효율성을 높여주는 추가 기법

운영체제는 페이징뿐 아니라 여러 기술을 함께 사용해요.

✔ 1) 가상 메모리(Virtual Memory)

RAM이 부족하면 디스크(Swap)를 활용
→ 실제 물리 메모리보다 더 큰 주소 공간 사용 가능

✔ 2) Demand Paging

필요한 페이지만 요청 시 메모리에 적재
→ 불필요한 메모리 낭비 줄임

✔ 3) 페이지 교체 알고리즘

LRU

LFU

Clock 알고리즘
→ 메모리가 꽉 찼을 때 어떤 페이지를 내릴지 결정

✔ 4) TLB(Translation Lookaside Buffer)

페이지 테이블을 캐싱하여 주소 변환 속도 개선

### 마지막 핵심 요약

페이징 = 동일 크기로 나누는 물리적 메모리 관리
세그멘테이션 = 의미 단위로 나누는 논리적 메모리 관리

페이징은 내부 단편화 문제를 가지지만 연속 공간 없이도 관리 가능→ 현대 OS의 기본 방식
세그멘테이션은 의미적 관리가 쉽고 보호에 좋지만 외부 단편화 문제 존재

## 25. 12. 2(화)
### 오늘의 질문: Stateful과 Stateless의 차이점은 무엇인가요?
### 1. Stateful이란?

서버가 클라이언트의 상태(State)를 기억하고 이어서 처리하는 방식

즉, 이전 요청에서 무슨 일이 있었는지 서버가 알고 있음.

예시 :

로그인 상태를 서버가 직접 들고 있음(Session)

쇼핑몰 장바구니 정보를 서버가 저장

채팅 연결 세션(WebSocket)이 서버에 유지

✔ 장점

이전 정보를 알고 있으므로 연속적인 서비스 제공에 유리

복잡한 비즈니스 로직에 강함

실시간 연결 기반 서비스(WebSocket 등)에서 자연스러움

❌ 단점

서버가 상태를 저장하므로 확장(Scaling)이 어려움

같은 사용자 요청을 “항상 같은 서버”로 보내야 함(세션 스티키 필요)

서버 장애 시 상태정보가 날아갈 위험

서버 리소스 많이 사용 (메모리·세션 저장소)

### 2. Stateless란?

서버가 클라이언트의 상태를 전혀 기억하지 않고,
모든 요청은 ‘독립된 요청’으로 처리하는 방식

즉, 매 요청은 새로 시작하는 것처럼 처리함.

예시 :

REST API

서버는 클라이언트가 누구인지, 전에 무엇을 했는지 저장하지 않음

인증 정보는 매번 JWT, Token 등으로 전달

✔ 장점

서버 확장(스케일링)에 매우 유리 → MSA의 기본

어느 서버가 처리해도 같음 → 로드밸런싱 쉬움

서버 메모리/상태 관리 부담 → 없음

❌ 단점

매 요청마다 필요한 정보(인증 등)를 담아야 → 데이터 크기 증가

연속적인 동작 구현이 복잡

상태가 필요한 서비스는 별도 저장소 필요(세션 DB, Redis 등)

###  3. 추가질문 : HTTP가 왜 Stateless 프로토콜일까?

HTTP는 원래 다음과 같은 철학을 가지고 설계됨:

“서버는 클라이언트 상태를 기억하지 않는다.
모든 요청은 독립적으로 처리되어야 한다.”

그 이유는:

간단하고 빠르게 통신하기 위해

서버의 부담을 줄이기 위해

대규모 확장성 확보를 위해

즉, 서버는

“누가 보냈는지”

“로그인되어 있는지”

“이전에 무엇을 요청했는지”
전혀 기억하지 않음.

그래서 로그인 세션, 쿠키, JWT 같은 기술이 등장한 것이에요.
HTTP 자체는 Stateless이지만, 우리가 필요한 경우 상태를 “서버 바깥”에서 관리하는 방식으로 확장하는 것.

### 4. Stateful vs Stateless 차이 간단 비교
| 구분         | Stateful            | Stateless             |
| ---------- | ------------------- | --------------------- |
| 서버가 상태 기억? | O                   | X                     |
| 확장성        | 낮음                  | 높음                    |
| 로드밸런싱      | 어려움(같은 서버 유지)       | 쉬움(아무 서버나 OK)         |
| 서버 장애 영향   | 큼 (상태 사라짐)          | 적음                    |
| 구현 난이도     | 간단                  | 토큰·세션처리 필요            |
| 사용 예       | 게임 서버, WebSocket 채팅 | REST API, 대부분의 웹앱 API |

### 5. 추가 질문 2 : 실제 서비스에서는 어떻게 할까?

대부분 Stateless를 기본으로 하고, 필요한 상태는 외부 저장소에 위임해요.

예:

로그인 상태 → Redis에 세션 저장

사용자 인증 → JWT 토큰 사용

장바구니 → DB 저장

채팅 연결 → 별도의 Stateful 서버(WebSocket)

즉, 비즈니스 로직 중심은 Stateless,
실시간성·연속성이 필요한 부분은 Stateful로 운영.

### 한 문장 요약

Stateful = 서버가 기억함 → 편하지만 확장이 어려움

Stateless = 서버가 기억 안 함 → 확장성 뛰어나고 REST API의 기본

## 25. 12. 03(수)
### 오늘의 질문: Scale Up(수직 확장)과 Scale Out(수평 확장)의 차이점은 무엇인가요?

### 1. Scale Up(수직 확장)이란?

기존 서버의 성능을 올리는 방식으로 확장하는 것

CPU 더 높은 걸로 업그레이드

RAM 증가

SSD 성능 향상

EC2 인스턴스를 t3 → m5 → c5 → r6 등급으로 변경

✔ 장점

구조 변경 없음 → 가장 쉽고 빠름

운영 고도화 필요 없음

단일 서버 기반 서비스에서 특히 효과적

❌ 단점

한계가 명확함 → 하드웨어는 무한정 커질 수 없음

비용 증가 매우 큼 (고성능일수록 비싸짐)

서버 하나가 죽으면 서비스 전체가 죽는 단일 장애점(SPoF) 문제

확장 속도 느림 (큰 인스턴스는 리스타트/업그레이드 필요)

### 2. Scale Out(수평 확장)이란?

서버 수를 늘려 여러 대가 같은 서비스를 처리하도록 확장하는 방식

EC2 1대 → 3대 → 10대

EKS에서 Pod 수 자동 증가(HPA)

Auto Scaling Group으로 서버 자동 증설

Redis Cluster, DB Sharding 등 분산 구조

✔ 장점

확장성이 매우 뛰어남 → “필요하면 서버 수를 계속 늘린다”

장애에 강함 → 한 서버 죽어도 서비스 유지

트래픽 분산(로드 밸런싱) 가능

클라우드·MSA 시대의 표준 방식

❌ 단점

아키텍처가 복잡해짐

로드 밸런서 필요

상태관리(세션, 캐시 등) 방식 고민 필요
→ Stateless 아키텍처가 중요해지는 이유!

DB 확장은 더 복잡 (수평 확장은 특히 어려움)

### 3. 추가 질문 : 실제 서비스에서 확장 결정을 어떻게 할까?
트래픽 증가할 때 일반적인 순서

1. Scale Up (짧은 시간, 단기 대응)

서버 사양 올리는 것이 가장 간단

급한 불 끄기 용으로도 빠름

2.  Scale Out (중장기)

로드밸런서 + 오토스케일링 구성

서버 수 자동 증가

시스템 전체를 확장 가능한 구조로 만들기

→ 대부분 회사들은 결국 수평 확장 구조로 넘어감
(EKS, Kubernetes, Serverless 등)

📌 4. 한눈에 비교하기
| 항목    | Scale Up (수직 확장)      | Scale Out (수평 확장)       |
| ----- | --------------------- | ----------------------- |
| 방식    | 서버 성능 업그레이드           | 서버 수 증가                 |
| 난이도   | 낮음(쉽고 빠름)             | 높음(구조 설계 필요)            |
| 확장 한계 | 있음                    | 거의 무한                   |
| 비용 효율 | 낮음                    | 높음                      |
| 장애 대응 | 단일 서버 장애 위험           | 분산 환경으로 안정적             |
| 요구 조건 | 단일 서버 중심 서비스          | Stateless/분산 시스템 필요     |
| 예시    | m5.large → m5.2xlarge | EC2 여러 대 + ALB, EKS HPA |

### 5. AWS 기준 실제 예시
✔ Scale Up

EC2 인스턴스를 더 큰 스펙으로 변경

RDS의 인스턴스를 작은 것 → 큰 것으로 변경

✔ Scale Out

Auto Scaling Group으로 EC2 자동 증설

Kubernetes HPA(K8s Horizontal Pod Autoscaler)

ALB + 여러 서버로 로드 밸런싱

Redis Cluster 샤딩

DynamoDB Auto Scaling

실무에서는 “Scale Out”이 훨씬 일반적임
클라우드가 등장한 이유 자체가 “수평 확장”을 쉽게 만들기 위함

⭐ 한 문장 요약

Scale Up = 서버 1대의 성능을 키우는 것

Scale Out = 서버 부대를 늘리는 것

둘 다 필요하지만,
장기적으로는 Scale Out이 현대 아키텍처의 핵심 전략임
